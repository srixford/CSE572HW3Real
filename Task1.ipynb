{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.512984Z",
     "start_time": "2025-11-25T02:49:24.826633Z"
    }
   },
   "source": [
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "data = np.loadtxt(\"data.csv\", delimiter=\",\")\n",
    "labels = np.loadtxt(\"label.csv\").astype(int)"
   ],
   "outputs": [],
   "execution_count": 417
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.521032Z",
     "start_time": "2025-11-25T02:49:25.518783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def euclideanDistance(x,y):\n",
    "    distance = np.sqrt(np.sum((x - y) ** 2))\n",
    "    return distance"
   ],
   "id": "29ba9e6cd4dc5804",
   "outputs": [],
   "execution_count": 418
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.526333Z",
     "start_time": "2025-11-25T02:49:25.524026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cosineDistance(x,y):\n",
    "    denominator = (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "    if (denominator == 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return (1 - (np.dot(x,y)))/denominator"
   ],
   "id": "d70e692f001889f4",
   "outputs": [],
   "execution_count": 419
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.531844Z",
     "start_time": "2025-11-25T02:49:25.529400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def jaccardDistance(x,y):\n",
    "    min = np.sum(np.minimum(x,y))\n",
    "    max = np.sum(np.maximum(x,y))\n",
    "\n",
    "    if(max == 0):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 - (min/max)"
   ],
   "id": "ae2d5a6ac46b2ebd",
   "outputs": [],
   "execution_count": 420
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.537325Z",
     "start_time": "2025-11-25T02:49:25.534893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def SSE(clusters, centroids):\n",
    "    error = 0\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        for index in cluster:\n",
    "            if len(cluster) > 0:\n",
    "                error += np.square(jaccardDistance(data[index], centroids[i]))\n",
    "    return error #/ (data.shape[0] * data.shape[1])\n",
    "#The above / (data.shape...) is for standardization of Euclidean distance since it ends up giving a way larger number than the other SSEs so that I can compare them easier."
   ],
   "id": "1a34cd5c2feb896f",
   "outputs": [],
   "execution_count": 421
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.543329Z",
     "start_time": "2025-11-25T02:49:25.540543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clusterLabels(clusters):\n",
    "    returnLabels = []\n",
    "\n",
    "    for cluster in clusters:\n",
    "        clusterCounts = []\n",
    "        for index in cluster:\n",
    "            clusterCounts.append(labels[index])\n",
    "\n",
    "        majority = Counter(clusterCounts).most_common(1)[0][0]\n",
    "        returnLabels.append(majority)\n",
    "\n",
    "    return returnLabels"
   ],
   "id": "bff250ba692b02b2",
   "outputs": [],
   "execution_count": 422
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.549191Z",
     "start_time": "2025-11-25T02:49:25.546707Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predictedLabels(returnLabels, centroids):\n",
    "    predictions = []\n",
    "    for x in data:\n",
    "        dists = [jaccardDistance(x, c) for c in centroids]\n",
    "        cluster_index = np.argmin(dists)\n",
    "        predictions.append(returnLabels[cluster_index])\n",
    "    return np.array(predictions)"
   ],
   "id": "1d46ab67a93474e",
   "outputs": [],
   "execution_count": 423
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.554605Z",
     "start_time": "2025-11-25T02:49:25.552387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def accuracy(pred, true):\n",
    "    return np.mean(pred == true)"
   ],
   "id": "9c6c9140becc1316",
   "outputs": [],
   "execution_count": 424
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.563614Z",
     "start_time": "2025-11-25T02:49:25.559220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def kmeans(k=10, iterations=100, seed=0):\n",
    "    np.random.seed(seed)\n",
    "    samples, features = data.shape\n",
    "    totalIterations = 0\n",
    "    previousSSE = float(\"inf\")\n",
    "\n",
    "    indices = np.random.choice(samples, k, replace=False)\n",
    "    centroids = data[indices].copy()\n",
    "    clusters = []\n",
    "\n",
    "    for blank in range(iterations):\n",
    "        clusters = [[] for i in range(k)]\n",
    "        for i in range(samples):\n",
    "            distances = []\n",
    "            for c in centroids:\n",
    "                distances.append(jaccardDistance(data[i], c))\n",
    "            clusters[np.argmin(distances)].append(i)\n",
    "\n",
    "        newCentroids = np.zeros_like(centroids)\n",
    "        for i in range(k):\n",
    "            if clusters[i]:\n",
    "                newCentroids[i] = np.mean(data[clusters[i]], axis=0)\n",
    "            else:\n",
    "                newCentroids[i] = centroids[i]\n",
    "\n",
    "        #Get rid of for Q4 when needed.\n",
    "        #if np.allclose(centroids, newCentroids):\n",
    "            #break\n",
    "\n",
    "        #Below is just for Q3 and also for Q4 when needed.\n",
    "        #if previousSSE < SSE(clusters, centroids):\n",
    "            #break\n",
    "        #else:\n",
    "            #previousSSE = SSE(clusters, centroids)\n",
    "\n",
    "        centroids = newCentroids\n",
    "        totalIterations += 1\n",
    "\n",
    "    return clusters, centroids, totalIterations"
   ],
   "id": "f65cd8fe17c20065",
   "outputs": [],
   "execution_count": 425
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.569696Z",
     "start_time": "2025-11-25T02:49:25.567524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##Testing for Questions\n",
    "##Q1 - went up to euclideanDistance and switched it for the other 2. also in SSE function.\n",
    "\n",
    "#clusters, centroids, iterations = kmeans()\n",
    "\n",
    "#SSE(clusters, centroids)"
   ],
   "id": "19d5c640384b7bbc",
   "outputs": [],
   "execution_count": 426
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "First value was 25321064456.818565 for Euclidean, but this also makes sense since it is not bounded the same way cosine and jaccard are. If I try to standardize it to be in align with the other two SSEs by diving by the total number of features * observations, I get 3229.7276092880825.\n",
    "Second was 5745.707845814075 for cosine\n",
    "Third was 3690.822622100952 for Jaccard\n",
    "\n",
    "This would make Jaccard the best method by going off of just normal SSEs like the question asked, however if we standardize Euclidean for easier comparison, it would have the best SSE."
   ],
   "id": "4c4dc669cfdb7a59"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.580148Z",
     "start_time": "2025-11-25T02:49:25.578077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##Q2 - same as Q1 where i just switch out where I run Euclidean distance for the other distance measurments.\n",
    "\n",
    "#clusters, centroids, iterations = kmeans()\n",
    "#returnLabels = clusterLabels(clusters)\n",
    "#predictLabels = predictedLabels(returnLabels, centroids)\n",
    "#accuracy(predictLabels, labels)"
   ],
   "id": "d3ebb5c56e88321",
   "outputs": [],
   "execution_count": 427
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For Euclidean distance, the accuracy is .5977.\n",
    "For Cosine distance the accuracy was 0.5478.\n",
    "For Jaccard, the accuracy was 0.5443.\n",
    "\n",
    "The best metric here was the Euclidean-k-means for accuracy."
   ],
   "id": "169011408c8ede28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:49:25.586039Z",
     "start_time": "2025-11-25T02:49:25.583979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##Q3 - same as Q1-2 where I just change the distance measurement in all spots for whichever one I am running. Also line 29 in kmeans function needs to be done for this to work as well.\n",
    "\n",
    "#clusters, centroids, iterations = kmeans()\n",
    "#iterations"
   ],
   "id": "1dd00acacd971505",
   "outputs": [],
   "execution_count": 428
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "For Jaccard, I got 11 iterations before it stopped, and it stopped via SSE value increasing in the next iteration.\n",
    "For cosine, it stopped after 1 iteration due to the SSE value increasing in the next iteration.\n",
    "For Euclidean it stopped after 47 iterations due to converging.\n",
    "\n",
    "I believe that this is due to the algorithmic way that SSE is computed for cosine distance and jaccard distance. When I ran without the SSE value increase in the next iteration I got 45 for cosine and 54 for jaccard.\n",
    "\n",
    "Still with all of the methods in place, Euclidean would take the most number of iterations and time to converge."
   ],
   "id": "c65358cc97528113"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T02:50:35.852503Z",
     "start_time": "2025-11-25T02:49:25.591311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "##Q4 - Similar to Q3 but I have more stopping points that I selected inside of kmeans that I have marked above.\n",
    "\n",
    "clusters, centroids, iterations = kmeans()\n",
    "\n",
    "SSE(clusters, centroids)\n"
   ],
   "id": "8a744d6266051a92",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3690.822622100952)"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 429
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "when there is no change in centroid position\n",
    "Euclidean: 3229.727\n",
    "Cosine: 5745.707\n",
    "Jaccard: 3690.822\n",
    "when the SSE value increases in the next iteration\n",
    "Euclidean: 3229.727\n",
    "Cosine: 5427.275\n",
    "Jaccard: 3686.075\n",
    "when the maximum preset value (which is 100) of iteration is complete\n",
    "Euclidean: 3229.727\n",
    "Cosine: 5745.707\n",
    "Jaccard: 3690.822\n",
    "\n",
    "Above, the values for Euclidean distance all make sense. Once there is no change in centroid position, there isn't going to be many changes in the SSE. Since converging happens first, the other values will be the same. For Cosine and Jaccard, because the values for SSE are computed based on a fixed scale, the lowest and highest values won't be that different. Also it is possible for the SSE values to go up and down for those two, the will have different values compared to the max iterations and when there is no change."
   ],
   "id": "e28f87d481ab708b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Q5:\n",
    "My observations were that first SSE is not the easiest to compare amongst the several different ways of K-means. I am not quite sure that it is a great method for determining which method is the best. Accuracy and the time/number of iterations seems best to me. The total number of iterations only was changed significantly when we stopped based off of SSE values. Outside of that it seems that we were able to have more consistent data when focusing on the other criteria. When we mainly focus on that, it seems that Euclidean performs the best with the other two performing slightly worse.\n",
    "Additionally, I noticed that when trying to label each cluster. that some clusters were assigned to the same centroid. This would cause accuracies to go down a bit. There are some ways that I could prevent this though like taught in class such as having more clusters and then picking the ones that are the furthest away from each other."
   ],
   "id": "36986a4963bf313f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
